[
  
    {
      "title"       : "node.js / 3rd-party ëª¨ë“ˆ",
      "category"    : "",
      "tags"        : "",
      "url"         : "./3rd-party-module.html",
      "date"        : "2023-01-24 00:00:00 +0900",
      "description" : "",
      "content"     : "í•™ìŠµëª©í‘œ Node.js ë‚´ì¥ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì„œë“œ íŒŒí‹° ëª¨ë“ˆ (3rd-party module)ì„ í™œìš©í•  ìˆ˜ ìˆë‹¤. Node.js ê³µì‹ ë¬¸ì„œë¥¼ í™œìš©í•˜ì—¬ fs.readFileì„ ì´ìš©í•  ìˆ˜ ìˆë‹¤. Node.jsğŸ“ê°œë…Node.jsëŠ” Chrome V8 JavaScript ì—”ì§„ìœ¼ë¡œ ë¹Œë“œ ëœ JavaScript ëŸ°íƒ€ì„ì´ë‹¤.ì¦‰, ë…¸ë“œë¥¼ í†µí•´ ë‹¤ì–‘í•œ ìë°”ìŠ¤í¬ë¦½íŠ¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìœ¼ë©°, ì„œë²„ë¥¼ ì‹¤í–‰í•˜ëŠ”ë° ë§ì´ ì´ìš©ë¨. ğŸƒ ìì‹ ì´ ì´í•´í•˜ëŠ” ë²”ìœ„ë§Œí¼ ëª¨ë“ˆì„ ì“¸ ìˆ˜ ìˆìŒ..ğŸ“ì‚¬ìš©ë²•íŒŒì¼ì„ ì½ì„ ë•Œ : readFile ì´ë¼ëŠ” ë©”ì„œë“œíŒŒì¼ì„ ì €ì¥í•  ë•Œ : writeFile ì´ë¼ëŠ” ë©”ì„œë“œ(saveFile ë©”ì„œë“œë¥¼ ì¨ì•¼í•  ê²ƒ ê°™ì§€ë§Œ ì—†ìŒ)ëª¨ë“  ëª¨ë“ˆì€ â€˜ëª¨ë“ˆì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë¶ˆëŸ¬ì˜¤ëŠ” ê³¼ì •â€™ì´ í•„ìš”&lt;script src=\"ë¶ˆëŸ¬ì˜¤ê³ ì‹¶ì€_ìŠ¤í¬ë¦½íŠ¸.js\"&gt;&lt;/script&gt;Node.js ì—ì„œëŠ” javascript ì½”ë“œ ê°€ì¥ ìƒë‹¨ì— require êµ¬ë¬¸ì„ ì´ìš©í•´ ë‹¤ë¥¸ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜´const fs = require('fs'); // íŒŒì¼ ì‹œìŠ¤í…œ ëª¨ë“ˆ í˜¸ì¶œconst dns = require('dns'); // DNS ëª¨ë“ˆ í˜¸ì¶œ// í˜¸ì¶œ í›„ fs.readFile ë©”ì„œë“œ ë“±ì„ ì‚¬ìš© ê°€ëŠ¥3rd-party ëª¨ë“ˆì„ ì‚¬ìš©í•˜ëŠ” ë²•ğŸ“ê°œë…í•´ë‹¹ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì—ì„œ ê³µì‹ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” ë¹ŒíŠ¸ì¸ ëª¨ë“ˆì´ ì•„ë‹Œ ëª¨ë“  ì™¸ë¶€ ëª¨ë“ˆì„ ì¼ì»«ëŠ”ë‹¤.ğŸ“ì‚¬ìš©ë²•ì˜ˆì‹œë¡œ underscoreì´ ìˆëŠ”ë° Node.js ê³µì‹ë¬¸ì„œì— ì—†ëŠ” ëª¨ë“ˆì´ì–´ì„œ ì„œë“œíŒŒí‹° ëª¨ë“ˆì„ â†’ ë‹¤ìš´ì„ ìœ„í•´ì„  npm ì‚¬ìš©í•´ì•¼í•¨npm install underscoreì´í›„ Node.js ë‚´ì¥ ëª¨ë“ˆì„ ì‚¬ìš©í•˜ë“¯ require êµ¬ë¬¸ì„ í†µí•´ í˜¸ì¶œ ê°€ëŠ¥const _ = require('underscore');ê³µì‹ ë¬¸ì„œ ì½ëŠ”ë²•fs.readFile(path[, options], callback)ë©”ì„œë“œ fs.readFileì€ ë¹„ë™ê¸°ì ìœ¼ë¡œ íŒŒì¼ ë‚´ìš© ì „ì²´ë¥¼ ì½ëŠ”ë‹¤.- path : &lt;string&gt; | &lt;Buffer&gt; | &lt;URL&gt; | &lt;integer&gt;pathì—ëŠ” íŒŒì¼ì´ë¦„ì„ ì „ë‹¬ì¸ìë¡œ ë°›ëŠ”ë°, ë„¤ ê°€ì§€ ì¢…ë¥˜ì˜ íƒ€ì…ì„ ë„˜ê¸¸ ìˆ˜ ìˆì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¬¸ìì—´ íƒ€ì… ì‚¬ìš©fs.readFile('/etc/passwd', ..., ...)- options : &lt;Object&gt; | &lt;string&gt;ëŒ€ê´„í˜¸ë¡œ ê°ì‹¼ ë‘ë²ˆì§¸ ì „ë‹¬ì¸ìëŠ” ì„ íƒì  ì „ë‹¬ì¸ìë¥¼ ì˜ë¯¸í•¨optionsì€ ë¬¸ìì—´ ë˜ëŠ” ê°ì²´ í˜•íƒœë¡œ ë°›ì„ ìˆ˜ ìˆë‹¤.ë¬¸ìì—´ë¡œ ì „ë‹¬í•  ê²½ìš° ì¸ì½”ë”©ì„ ë°›ìŒ.// /etc/passwd íŒŒì¼ì„ 'utf8'ì„ ì‚¬ìš©í•˜ì—¬ ì½ìŠµë‹ˆë‹¤.fs.readFile('/etc/passwd', 'utf8', ...);let options = { encoding: 'utf8', // utf8 ì¸ì½”ë”© ë°©ì‹ìœ¼ë¡œ ì—½ë‹ˆë‹¤ flag: 'r' // ì½ê¸° ìœ„í•´ ì—½ë‹ˆë‹¤}// /etc/passwd íŒŒì¼ì„ optionsë¥¼ ì‚¬ìš©í•˜ì—¬ ì½ìŠµë‹ˆë‹¤.fs.readFile('/etc/passwd', options, ...)- callback &lt;Function&gt;ë§ˆì§€ë§‰ì¸ìëŠ” ì½œë°±í•¨ìˆ˜ë¥¼ ì „ë‹¬í•¨. íŒŒì¼ì„ ì½ê³  ë‚œ í›„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜ err &lt;Error&gt; | &lt;AggregateError&gt; data &lt;string&gt; | &lt;Buffer&gt;ì½œë°±í•¨ìˆ˜ì—ëŠ” ë‘ê°€ì§€ ë§¤ê°œë³€ìˆ˜ ìˆëŠ”ë°, ì—ëŸ¬ ë°œìƒí•˜ì§€ ì•Šìœ¼ë©´ errì€ nullì´ ë˜ë©°, dataì— ë¬¸ìì—´ì´ë‚˜ Buffer ë¼ëŠ” ê°ì²´ê°€ ì „ë‹¬ë¨.fs.readFile('test.txt', 'utf8', (err, data) =&gt; { if (err) { throw err; // ì—ëŸ¬ë¥¼ ë˜ì§‘ë‹ˆë‹¤. } console.log(data);});"
    } ,
  
    {
      "title"       : "TIL START",
      "category"    : "",
      "tags"        : "TIL",
      "url"         : "./TIL-Start.html",
      "date"        : "2023-01-23 00:00:00 +0900",
      "description" : "ê¹ƒ ë¸”ë¡œê·¸ ë§Œë“œëŠ”ê±° ì™œì´ë ‡ê²Œ ì–´ë ¤ì›Œìš”.",
      "content"     : "TIL ë¸”ë¡œê·¸ ì…ë‹ˆë‹¤.ì•ˆë…•í•˜ì„¸ìš”. github ë¸”ë¡œê·¸ ë§Œë“¤ê¸° í˜ë“œë„¤ìš”.ì¬ì—…ë¡œë“œì…ë‹ˆë‹¤â€¦.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse"
    } ,
  
    {
      "title"       : "Conway&#39;s Game of Life",
      "category"    : "",
      "tags"        : "coding, python",
      "url"         : "./conways-game-of-life.html",
      "date"        : "2021-02-11 04:32:20 +0900",
      "description" : "Taking on the challenge of picking up coding again through interesting small projects, this time it is the turn of Conway's Game of Life.",
      "content"     : "I&nbsp;am lately trying to take on coding again. It had always been a part of my life since my early years when I&nbsp;learned to program a Tandy Color Computer at the age of 8, the good old days.Tandy Color Computer TRS80 IIIHaving already programed in Java, C# and of course BASIC, I&nbsp;thought it would be a great idea to learn Python since I&nbsp;have great interest in data science and machine learning, and those two topics seem to have an avid community within Python coders.For one of my starter quick programming tasks, I&nbsp;decided to code Conway's Game of Life, a very simple cellular automata that basically plays itself.The game consists of a grid of n size, and within each block of the grid a cell could either be dead or alive according to these rules:If a cell has less than 2 neighbors, meaning contiguous alive cells, the cell will die of lonelinessIf a cell has more than 3 neighbors, it will die of overpopulationIf an empty block has exactly 3 contiguous alive neighbors, a new cell will be born in that spotIf an alive cell has 2 or 3 alive neighbors, it continues to liveConwayâ€™s rules for the Game of LifeTo make it more of a challenge I&nbsp;also decided to implement an \"sparse\" method of recording the game board, this means that instead of the typical 2d array representing the whole board, I&nbsp;will only record the cells which are alive. Saving a lot of memory space and processing time, while adding some spice to the challenge.The trickiest part was figuring out how to calculate which empty blocks had exactly 3 alive neighbors so that a new cell will spring to life there, this is trivial in the case of recording the whole grid, because we just iterate all over the board and find the alive neighbors of ALL&nbsp;the blocks in the grid, but in the case of only keeping the alive cells proved quite a challenge.In the end the algorithm ended up as follows:Iterate through all the alive cells and get all of their neighborsdef get_neighbors(self, cell): neighbors = [] for x in range(-1, 2, 1): for y in range(-1, 2, 1): if not (x == 0 and y == 0): if (0 &amp;lt;= (cell[0] + x) &amp;lt;= self.size_x) and (0 &amp;lt;= (cell[1] + y) &amp;lt;= self.size_y): neighbors.append((cell[0] + x, cell[1] + y)) return neighborsMark all the neighboring blocks as having +1 neighbor each time a particular cell is encountered. This way, for each neighboring alive cell the counter of the particular block will increase, and in the end it will contain the total number of live cells which are contiguous to it.def next_state(self): alive_neighbors = {} for cell in self.alive_cells: if cell not in alive_neighbors: alive_neighbors[cell] = 0 neighbors = self.get_neighbors(cell) for neighbor in neighbors: if neighbor not in alive_neighbors: alive_neighbors[neighbor] = 1 else: alive_neighbors[neighbor] += 1The trick was using a dictionary to keep the record of the blocks that have alive neighbors and the cells who are alive in the current state but have zero alive neighbors (thus will die).With the dictionary it became easy just to add cells and increase their neighbor counter each time it was encountered as a neighbor of an alive cell.Having the dictionary now filled with all the cells that have alive neighbors and how many they have, it was just a matter of applying the rules of the game:for cell in alive_neighbors: if alive_neighbors[cell] &amp;lt; 2 or alive_neighbors[cell] &gt; 3: self.alive_cells.discard(cell) elif alive_neighbors[cell] == 3: self.alive_cells.add(cell)Notice that since I am keeping an array of the coordinates of only the cells who are alive, I could apply just 3 rules, die of loneliness, die of overpopulation and become alive from reproduction (exactly 3 alive neighbors) because the ones who have 2 or 3 neighbors and are already alive, can remain alive in the next iteration.I&nbsp;found it very interesting to implement the Game of Life like this, it was quite a refreshing challenge and I am beginning to feel my coding skills ramping up again."
    } ,
  
    {
      "title"       : "Single Neuron Perceptron",
      "category"    : "",
      "tags"        : "machine learning, coding, neural networks",
      "url"         : "./single-neuron-perceptron.html",
      "date"        : "2021-01-26 04:32:20 +0900",
      "description" : "Single neuron perceptron that classifies elements learning quite quickly.",
      "content"     : "As an entry point to learning python and getting into Machine Learning, I decided to code from scratch the Hello World! of the field, a single neuron perceptron.What is a perceptron?A perceptron is the basic building block of a neural network, it can be compared to a neuron, And its conception is what detonated the vast field of Artificial Intelligence nowadays.Back in the late 1950â€™s, a young Frank Rosenblatt devised a very simple algorithm as a foundation to construct a machine that could learn to perform different tasks.In its essence, a perceptron is nothing more than a collection of values and rules for passing information through them, but in its simplicity lies its power.Imagine you have a â€˜neuronâ€™ and to â€˜activateâ€™ it, you pass through several input signals, each signal connects to the neuron through a synapse, once the signal is aggregated in the perceptron, it is then passed on to one or as many outputs as defined. A perceptron is but a neuron and its collection of synapses to get a signal into it and to modify a signal to pass on.In more mathematical terms, a perceptron is an array of values (letâ€™s call them weights), and the rules to apply such values to an input signal.For instance a perceptron could get 3 different inputs as in the image, lets pretend that the inputs it receives as signal are: $x_1 = 1, \\; x_2 = 2\\; and \\; x_3 = 3$, if itâ€™s weights are $w_1 = 0.5,\\; w_2 = 1\\; and \\; w_3 = -1$ respectively, then what the perceptron will do when the signal is received is to multiply each input value by its corresponding weight, then add them up.\\(\\begin{align}\\begin{split}\\left(x_1 * w_1\\right) + \\left(x_2 * w_2\\right) + \\left(x_3 * w_3\\right)\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}\\left(0.5 * 1\\right) + \\left(1 * 2\\right) + \\left(-1 * 3\\right) = 0.5 + 2 - 3 = -0.5\\end{split}\\end{align}\\)Typically when this value is obtained, we need to apply an â€œactivationâ€ function to smooth the output, but letâ€™s say that our activation function is linear, meaning that we keep the value as it is, then thatâ€™s it, that is the output of the perceptron, -0.5.In a practical application, the output means something, perhaps we want our perceptron to classify a set of data and if the perceptron outputs a negative number, then we know the data is of type A, and if it is a positive number then it is of type B.Once we understand this, the magic starts to happen through a process called backpropagation, where we â€œeducateâ€ our tiny one neuron brain to have it learn how to do its job.The magic starts to happen through a process called backpropagation, where we \"educate\" our tiny one neuron brain to have it learn how to do its job.For this we need a set of data that it is already classified, we call this a training set. This data has inputs and their corresponding correct output. So we can tell the little brain when it misses in its prediction, and by doing so, we also adjust the weights a bit in the direction where we know the perceptron committed the mistake hoping that after many iterations like this the weights will be so that most of the predictions will be correct.After the model trains successfully we can have it classify data it has never seen before, and we have a fairly high confidence that it will do so correctly.The math behind this magical property of the perceptron is called gradient descent, and is just a bit of differential calculus that helps us convert the error the brain is having into tiny nudges of value of the weights towards their optimum. This video series by 3 blue 1 brown explains it wonderfuly.My program creates a single neuron neural network tuned to guess if a point is above or below a randomly generated line and generates a visualization based on graphs to see how the neural network is learning through time.The neuron has 3 inputs and weights to calculate its output:input 1 is the X coordinate of the point,Input 2 is the y coordinate of the point,Input 3 is the bias and it is always 1Input 3 or the bias is required for lines that do not cross the origin (0,0)The Perceptron starts with weights all set to zero and learns by using 1,000 random points per each iteration.The output of the perceptron is calculated with the following activation function: if x * weight_x + y weight_y + weight_bias is positive then 1 else 0The error for each point is calculated as the expected outcome of the perceptron minus the real outcome therefore there are only 3 possible error values: Expected Calculated Error 1 -1 1 1 1 0 -1 -1 0 -1 1 -1 With every point that is learned if the error is not 0 the weights are adjusted according to:New_weight = Old_weight + error * input * learning_ratefor example: New_weight_x = Old_weight_x + error * x * learning rateA very useful parameter in all of neural networks is teh learning rate, which is basically a measure on how tiny our nudge to the weights is going to be.In this particular case, I coded the learning_rate to decrease with every iteration as follows:learning_rate = 0.01 / (iteration + 1)this is important to ensure that once the weights are nearing the optimal values the adjustment in each iteration is subsequently more subtle.In the end, the perceptron always converges into a solution and finds with great precision the line we are looking for.Perceptrons are quite a revelation in that they can resolve equations by learning, however they are very limited. By their nature they can only resolve linear equations, so their problem space is quite narrow.Nowadays the neural networks consist of combinations of many perceptrons, in many layers, and other types of â€œneuronsâ€, like convolution, recurrent, etc. increasing significantly the types of problems they solve."
    } 
  
]
